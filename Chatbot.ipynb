{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intents and patterns \n",
    "with open('pattern.json') as json_data:\n",
    "    intents= json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre_processing 1\n",
    "classes= []\n",
    "words= []\n",
    "documents= []\n",
    "ignore_words= ['?','.',',','!']\n",
    "stemmer= LancasterStemmer()\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w= nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words= [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words= sorted(list(set(words)))\n",
    "classes= sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_processing 2\n",
    "training= []\n",
    "output_empty= np.zeros(len(classes))\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words= doc[0]\n",
    "    pattern_words= [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    output_row= list(output_empty)\n",
    "    output_row[classes.index(doc[1])]= 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training= np.array(training)\n",
    "\n",
    "x_train= list(training[:,0])\n",
    "y_train= list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deap learning model with optimized h_parameters \n",
    "try:\n",
    "    model= load_model('cbot.h5')\n",
    "except:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape= (len(x_train[0]),), activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(y_train[0]), activation= 'softmax'))\n",
    "    sgd= SGD(lr= 0.01, decay= 1e-6, momentum= 0.9, nesterov= True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= sgd, metrics= ['accuracy'])\n",
    "    model.fit(np.array(x_train), np.array(y_train), epochs= 200, batch_size= 5, verbose= 0)\n",
    "    model.save('cbot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bag of words\n",
    "def BoW(sents,words):\n",
    "    Bag= np.zeros(len(words))\n",
    "    SW= nltk.word_tokenize(sents)\n",
    "    SW= [stemmer.stem(w.lower()) for w in SW if w not in ignore_words]\n",
    "    for _ in SW:\n",
    "         for i,j in enumerate(words):\n",
    "                if _ == j:\n",
    "                    Bag[i]= 1\n",
    "    Bag=np.reshape(Bag,(1,69))\n",
    "    return np.array(Bag)\n",
    "#a function to find a string sector in a data frame \n",
    "def sec_finder(df,clss,sector):\n",
    "    a=[]\n",
    "    for i in range(1,len(df.columns)):\n",
    "        try:\n",
    "            b=nltk.word_tokenize(df.iloc[clss,i])\n",
    "            if sector in b:\n",
    "                a.append([df.iloc[clss,i], df.columns[i]])\n",
    "        except:\n",
    "            continue\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatbot body\n",
    "def chat():\n",
    "    print('My name is Robo and I am a chatbot! You can leave the chat by typing: quit')\n",
    "    name= input('What is your name?  ',)\n",
    "    print('@[*-*]@ : Hi '+name, ' How are you? ')\n",
    "    threshold= 0.5\n",
    "    schl= pd.read_csv('School.csv')\n",
    "    school= None \n",
    "    \n",
    "    while True:\n",
    "        inpt= input('\\n'+name+ ':  ')\n",
    "        if inpt== 'quit':\n",
    "            break\n",
    "        result= model.predict([BoW(inpt,words)])\n",
    "        indx= np.argmax(result)\n",
    "        tag= classes[indx]\n",
    "        for t in intents['intents']:\n",
    "            if tag== t['tag'] and (np.max(result)- np.mean(result)> threshold):\n",
    "                print('@[*-*]@ : ', random.choice(t['responses']))\n",
    "                if tag in ['help','Next_Event', 'deadline', 'Exam', 'Lecture'] and school ==None:\n",
    "                    #while school is None:\n",
    "                        try:\n",
    "                            school= int(input('@[*-*]@: please provide your school number from this list: \\n1.IT \\n2.Business \\n3.Art \\n4.Engineering \\n5.Law\\n'))-1\n",
    "                        except:\n",
    "                            print('@[*-*]@: sorry, it is not a valid number. I may ask you later to put it again')\n",
    "                if tag== 'Next_Event':\n",
    "                    fst_ev= next(i for i, j in enumerate([schl.iloc[school,b] for b in range(1,13)]) if type(j)==str)+1\n",
    "                    print('@[*-*]@ : The closest one is: ', schl.iloc[(school),fst_ev], ' on ',schl.columns[fst_ev])\n",
    "                elif tag== 'deadline':\n",
    "                    print('@[*-*]@ : The deadline for assignment submission is ', ('11/2/13'if school !=3 else '12/2/13'))\n",
    "                elif tag== 'Exam':\n",
    "                    print('@[*-*]@ : your school exams scheduled for this semester is/are: ', sec_finder(schl,school,\"Exam\")  )\n",
    "                elif tag== 'Lecture':\n",
    "                    print('@[*-*]@ : your school Lecture scheduled for this semester is/are: ', sec_finder(schl,school,\"Lecture\")  )\n",
    "            elif tag== t['tag'] and (np.max(result)- np.mean(result)< threshold):\n",
    "                print('@[*-*]@ :I do not understand, would you please make it more clear')\n",
    "    return        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
